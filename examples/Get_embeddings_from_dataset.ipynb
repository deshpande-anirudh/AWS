{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmZhc_SvNrw"
      },
      "source": [
        "# Get embeddings from dataset\n",
        "\n",
        "This notebook gives an example on how to get embeddings from a large dataset.\n",
        "\n",
        "\n",
        "## 1. Load the dataset\n",
        "\n",
        "The dataset used in this example is [fine-food reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews) from Amazon. The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of this dataset, consisting of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).\n",
        "\n",
        "We will combine the review summary and review text into a single combined text. The model will encode this combined text and it will output a single vector embedding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap as tr\n",
        "from typing import List, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from scipy import spatial\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "my_secret = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = my_secret\n",
        "\n",
        "client = OpenAI(max_retries=5)\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model=\"text-embedding-3-small\", **kwargs) -> List[float]:\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    response = client.embeddings.create(input=[text], model=model, **kwargs)\n",
        "\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "async def aget_embedding(\n",
        "    text: str, model=\"text-embedding-3-small\", **kwargs\n",
        ") -> List[float]:\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    return (await client.embeddings.create(input=[text], model=model, **kwargs))[\n",
        "        \"data\"\n",
        "    ][0][\"embedding\"]\n",
        "\n",
        "\n",
        "def get_embeddings(\n",
        "    list_of_text: List[str], model=\"text-embedding-3-small\", **kwargs\n",
        ") -> List[List[float]]:\n",
        "    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\n",
        "\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    list_of_text = [text.replace(\"\\n\", \" \") for text in list_of_text]\n",
        "\n",
        "    data = client.embeddings.create(input=list_of_text, model=model, **kwargs).data\n",
        "    return [d.embedding for d in data]\n",
        "\n",
        "\n",
        "async def aget_embeddings(\n",
        "    list_of_text: List[str], model=\"text-embedding-3-small\", **kwargs\n",
        ") -> List[List[float]]:\n",
        "    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\n",
        "\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    list_of_text = [text.replace(\"\\n\", \" \") for text in list_of_text]\n",
        "\n",
        "    data = (\n",
        "        await client.embeddings.create(input=list_of_text, model=model, **kwargs)\n",
        "    ).data\n",
        "    return [d.embedding for d in data]\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "\n",
        "def plot_multiclass_precision_recall(\n",
        "    y_score, y_true_untransformed, class_list, classifier_name\n",
        "):\n",
        "    \"\"\"\n",
        "    Precision-Recall plotting for a multiclass problem. It plots average precision-recall, per class precision recall and reference f1 contours.\n",
        "\n",
        "    Code slightly modified, but heavily based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
        "    \"\"\"\n",
        "    n_classes = len(class_list)\n",
        "    y_true = pd.concat(\n",
        "        [(y_true_untransformed == class_list[i]) for i in range(n_classes)], axis=1\n",
        "    ).values\n",
        "\n",
        "    # For each class\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    average_precision = dict()\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], y_score[:, i])\n",
        "        average_precision[i] = average_precision_score(y_true[:, i], y_score[:, i])\n",
        "\n",
        "    # A \"micro-average\": quantifying score on all classes jointly\n",
        "    precision_micro, recall_micro, _ = precision_recall_curve(\n",
        "        y_true.ravel(), y_score.ravel()\n",
        "    )\n",
        "    average_precision_micro = average_precision_score(y_true, y_score, average=\"micro\")\n",
        "    print(\n",
        "        str(classifier_name)\n",
        "        + \" - Average precision score over all classes: {0:0.2f}\".format(\n",
        "            average_precision_micro\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # setup plot details\n",
        "    plt.figure(figsize=(9, 10))\n",
        "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "    lines = []\n",
        "    labels = []\n",
        "    for f_score in f_scores:\n",
        "        x = np.linspace(0.01, 1)\n",
        "        y = f_score * x / (2 * x - f_score)\n",
        "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
        "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "    lines.append(l)\n",
        "    labels.append(\"iso-f1 curves\")\n",
        "    (l,) = plt.plot(recall_micro, precision_micro, color=\"gold\", lw=2)\n",
        "    lines.append(l)\n",
        "    labels.append(\n",
        "        \"average Precision-recall (auprc = {0:0.2f})\" \"\".format(average_precision_micro)\n",
        "    )\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        (l,) = plt.plot(recall[i], precision[i], lw=2)\n",
        "        lines.append(l)\n",
        "        labels.append(\n",
        "            \"Precision-recall for class `{0}` (auprc = {1:0.2f})\"\n",
        "            \"\".format(class_list[i], average_precision[i])\n",
        "        )\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    fig.subplots_adjust(bottom=0.25)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"{classifier_name}: Precision-Recall curve for each class\")\n",
        "    plt.legend(lines, labels)\n",
        "\n",
        "\n",
        "def distances_from_embeddings(\n",
        "    query_embedding: List[float],\n",
        "    embeddings: List[List[float]],\n",
        "    distance_metric=\"cosine\",\n",
        ") -> List[List]:\n",
        "    \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\n",
        "    distance_metrics = {\n",
        "        \"cosine\": spatial.distance.cosine,\n",
        "        \"L1\": spatial.distance.cityblock,\n",
        "        \"L2\": spatial.distance.euclidean,\n",
        "        \"Linf\": spatial.distance.chebyshev,\n",
        "    }\n",
        "    distances = [\n",
        "        distance_metrics[distance_metric](query_embedding, embedding)\n",
        "        for embedding in embeddings\n",
        "    ]\n",
        "    return distances\n",
        "\n",
        "\n",
        "def indices_of_nearest_neighbors_from_distances(distances) -> np.ndarray:\n",
        "    \"\"\"Return a list of indices of nearest neighbors from a list of distances.\"\"\"\n",
        "    return np.argsort(distances)\n",
        "\n",
        "\n",
        "def pca_components_from_embeddings(\n",
        "    embeddings: List[List[float]], n_components=2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Return the PCA components of a list of embeddings.\"\"\"\n",
        "    pca = PCA(n_components=n_components)\n",
        "    array_of_embeddings = np.array(embeddings)\n",
        "    return pca.fit_transform(array_of_embeddings)\n",
        "\n",
        "\n",
        "def tsne_components_from_embeddings(\n",
        "    embeddings: List[List[float]], n_components=2, **kwargs\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Returns t-SNE components of a list of embeddings.\"\"\"\n",
        "    # use better defaults if not specified\n",
        "    if \"init\" not in kwargs.keys():\n",
        "        kwargs[\"init\"] = \"pca\"\n",
        "    if \"learning_rate\" not in kwargs.keys():\n",
        "        kwargs[\"learning_rate\"] = \"auto\"\n",
        "    tsne = TSNE(n_components=n_components, **kwargs)\n",
        "    array_of_embeddings = np.array(embeddings)\n",
        "    return tsne.fit_transform(array_of_embeddings)\n",
        "\n",
        "\n",
        "def chart_from_components(\n",
        "    components: np.ndarray,\n",
        "    labels: Optional[List[str]] = None,\n",
        "    strings: Optional[List[str]] = None,\n",
        "    x_title=\"Component 0\",\n",
        "    y_title=\"Component 1\",\n",
        "    mark_size=5,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"Return an interactive 2D chart of embedding components.\"\"\"\n",
        "    empty_list = [\"\" for _ in components]\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            x_title: components[:, 0],\n",
        "            y_title: components[:, 1],\n",
        "            \"label\": labels if labels else empty_list,\n",
        "            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\n",
        "            if strings\n",
        "            else empty_list,\n",
        "        }\n",
        "    )\n",
        "    chart = px.scatter(\n",
        "        data,\n",
        "        x=x_title,\n",
        "        y=y_title,\n",
        "        color=\"label\" if labels else None,\n",
        "        symbol=\"label\" if labels else None,\n",
        "        hover_data=[\"string\"] if strings else None,\n",
        "        **kwargs,\n",
        "    ).update_traces(marker=dict(size=mark_size))\n",
        "    return chart\n",
        "\n",
        "\n",
        "def chart_from_components_3D(\n",
        "    components: np.ndarray,\n",
        "    labels: Optional[List[str]] = None,\n",
        "    strings: Optional[List[str]] = None,\n",
        "    x_title: str = \"Component 0\",\n",
        "    y_title: str = \"Component 1\",\n",
        "    z_title: str = \"Compontent 2\",\n",
        "    mark_size: int = 5,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"Return an interactive 3D chart of embedding components.\"\"\"\n",
        "    empty_list = [\"\" for _ in components]\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            x_title: components[:, 0],\n",
        "            y_title: components[:, 1],\n",
        "            z_title: components[:, 2],\n",
        "            \"label\": labels if labels else empty_list,\n",
        "            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\n",
        "            if strings\n",
        "            else empty_list,\n",
        "        }\n",
        "    )\n",
        "    chart = px.scatter_3d(\n",
        "        data,\n",
        "        x=x_title,\n",
        "        y=y_title,\n",
        "        z=z_title,\n",
        "        color=\"label\" if labels else None,\n",
        "        symbol=\"label\" if labels else None,\n",
        "        hover_data=[\"string\"] if strings else None,\n",
        "        **kwargs,\n",
        "    ).update_traces(marker=dict(size=mark_size))\n",
        "    return chart"
      ],
      "metadata": {
        "id": "7_JLwKN9vdsN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3iuFLlVvNry"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OW2WRl_pvbSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VsZYbYy-vNrz"
      },
      "outputs": [],
      "source": [
        "embedding_model = \"text-embedding-3-small\"\n",
        "embedding_encoding = \"cl100k_base\"\n",
        "max_tokens = 8000  # the maximum for text-embedding-3-small is 8191"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcxnlkmxvNry"
      },
      "source": [
        "To run this notebook, you will need to install: pandas, openai, transformers, plotly, matplotlib, scikit-learn, torch (transformer dep), torchvision, and scipy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "X8B-TEOYvNrz",
        "outputId": "360670c1-39a4-49b1-fda1-1e061315f9f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/fine_food_reviews_1k.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6626a3090b79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load & inspect dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_datapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/fine_food_reviews_1k.csv\"\u001b[0m  \u001b[0;31m# to save space, we provide a pre-filtered dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_datapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProductId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"UserId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fine_food_reviews_1k.csv'"
          ]
        }
      ],
      "source": [
        "# load & inspect dataset\n",
        "input_datapath = \"data/fine_food_reviews_1k.csv\"  # to save space, we provide a pre-filtered dataset\n",
        "df = pd.read_csv(input_datapath, index_col=0)\n",
        "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
        "df = df.dropna()\n",
        "df[\"combined\"] = (\n",
        "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
        ")\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qm3CLpQvNrz",
        "outputId": "91a97909-573c-4f52-d60e-1b2cb68a0b08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# subsample to 1k most recent reviews and remove samples that are too long\n",
        "top_n = 1000\n",
        "df = df.sort_values(\"Time\").tail(top_n * 2)  # first cut to first 2k entries, assuming less than half will be filtered out\n",
        "df.drop(\"Time\", axis=1, inplace=True)\n",
        "\n",
        "encoding = tiktoken.get_encoding(embedding_encoding)\n",
        "\n",
        "# omit reviews that are too long to embed\n",
        "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
        "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja43Yim3vNrz"
      },
      "source": [
        "## 2. Get embeddings and save them for future reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzNJ46AuvNrz"
      },
      "outputs": [],
      "source": [
        "# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n",
        "\n",
        "# This may take a few minutes\n",
        "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, model=embedding_model))\n",
        "df.to_csv(\"data/fine_food_reviews_with_embeddings_1k.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSesH95zvNr1"
      },
      "outputs": [],
      "source": [
        "a = get_embedding(\"hi\", model=embedding_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}